<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[虚拟机安装]]></title>
    <url>%2F2019%2F09%2F15%2Fvirtual-machine-setting%2F</url>
    <content type="text"><![CDATA[虚拟机安装及与主机间的文件共享啾咪，这是一篇没有图片的教程，额呵呵……咩有图片，大概是因为，懒吧……想要安装虚拟机是因为很多软件没有 Linux 版本，比如公司常用的钉钉，同时没有百度网盘客户端根本没办法下载一些大文件，虽然说现在百度网盘出了 Linux 版本，但是呵呵哒，根本不能用。而想要利用虚拟机下载文件，那就需要解决如何把虚拟机文件共享到主机的问题。 虚拟机安装使用工具： VirtualBox VMware 上面两个是常见的虚拟机安装的软件，最后还是选择了 VirtualBox，原因有下面几个 我需要虚拟机实现的功能不多，VirtualBox 相对来说安装更便捷； VMware 需要付费，虽然它也有免费版可以使用，但就总觉得不得劲儿； 刚开始是准备用 VMware 的，但是在启动虚拟机是遇到了问题，而在尝试使用 VirtualBox 时我有顺手把这个问题解决了，哈哈 Anyway，其实两个软件的使用操作都是大同小异。首先需要安装好 VirtualBox， Linux 下安装可以直接使用命令行 sudo apt install virtualbox ；如果是 Windows ，那就直接下载安装～安装好之后，点击新建虚拟机，设置好名字，系统，想要分配的大小，虚拟硬盘文件类型选择磁盘映像。创建好虚拟机后，选中虚拟机，点击启动，这时候就需要选定下载好的光盘映像文件(.iso)。之后，就按照正常的系统安装过程进行就 OK 啦～ 在启动虚拟机时，可能会出现这样的报错信息： 本机支持 VT-x ，但 VT-x 未开启。 解决办法就是重启电脑进入 BIOS 模式将它打开。细节问题，比如怎么进入 BIOS 模式？在哪里打开 VT-x ？ 不同电脑按键不一样，一般是在看到系统图标时按 F2 / F8 / F9 / F11 / F12 ，就直接试试或是百度吧。 关于第二点，就，多找找总能找到的，不然，也可以继续百度呀。 虚拟机与主机的文件共享这里基于 VirtualBox ，跟 VMware 的区别就有些大了。以下介绍的是使用共享文件夹进行文件共享的方式。 首先，启动需要共享的虚拟机，左上方点击设备-安装增强功能，下载好驱动后，安装它（不同系统对应有不同的安装包）。接下来新建共享文件夹，左上方点击控制-设置-共享文件夹，点击右侧带加号的文件夹图标新建，选择想要共享的文件夹，并给它命名。会有三个选项：只读分配，自动挂载，固定分配。如果想要虚拟机可以修改共享文件夹内容（写入），就不要勾选只读分配；之后的自动分配和固定挂载，为了方便我都会勾选。]]></content>
      <categories>
        <category>大杂烩</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[群晖 NAS 远程访问]]></title>
    <url>%2F2019%2F09%2F15%2Fsynology-NAS-remote-access%2F</url>
    <content type="text"><![CDATA[外部访问，根据引导教程进行设置。 确保网络连接所用的 IP （可百度‘我的 IP ’查看）和 公网 IP 一致 ！！！ 路由器添加映射端口。本机端口 ＝ 内部端口路由器端口 ＝ 外部端口IP 地址 ＝ NAS 在局域网内部的 IP]]></content>
      <categories>
        <category>大杂烩</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python_virtual_enviroments]]></title>
    <url>%2F2019%2F07%2F28%2Fpython-virtual-enviroments%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[对异或运算(xor)的探索]]></title>
    <url>%2F2019%2F05%2F16%2F%E5%AF%B9%E5%BC%82%E6%88%96%E8%BF%90%E7%AE%97%E7%9A%84%E6%8E%A2%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[由来开始注意到这个问题是因为在 leetcode 上面刷到了这样一道题——只出现一次的数字。题目要求为：给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次，找出那个只出现了一次的元素。需要注意的是：算法应该具有线性时间复杂度，尽量不使用额外的空间。当时看到这个题，想的思路是：对题目中给出的数组进行遍历，用一个新的数组 N 来存储这些数，如果某个数不存在 N 中，那么就把这个数存进去，如果已经存在，就将 N 中的这个数删除，最后 N 中剩下的一个数就是那个唯一只出现一次的数了。上面的方法可以实现功能，但是时间复杂度和空间复杂度都比较高，很不划算，然后，我就去看了一下已有的题解，先是很懵逼，之后就觉得非常巧妙了，代码如下：123456class Solution: def singleNumber(self, nums: List[int]) -&gt; int: result = 0 for i in range(len(nums)): result ^= nums[i] return result 以上即为，假如给出的数组是这样的：[a, b, p, m, b, m, a] ，代码中最终的结果也就是result = a^b^p^m^b^m^a = p，能够得到这样的答案，那么异或运算肯定就符合交换律，并且a^a = 0; a^0 = 0。然后我就想，这么神奇的东西，我一定要弄清楚。所以，以下就是，什么是异或运算？异或运算都哪些性质，为什么？以及，异或运算都可以用来做什么？ 异或运算异或运算，其实就是一种逻辑运算，$p$ 异或 $q$ 记作 $pXORq$ ，在 python 中写作p^q，其真值运算表如下： p $T$ $T$ $F$ $F$ q $T$ $F$ $T$ $F$ p ^ q $F$ $T$ $T$ $F$ 查了维基百科上面还有一些使用 且、或、非 来表达异或关系的表达式(真的不想敲数学公式)。 对两个数进行异或运算时，可以先把两个数转换为二进制形式，然后对其按位进行异或操作，即可得到最后的答案，比如下例： p $1$ $0$ $0$ $1$ $1$ $0$ $1$ $1$ q $0$ $0$ $1$ $0$ $1$ $1$ $1$ $0$ p ^ q 1 0 1 1 0 1 0 1 性质 and why ? 交换律：$p$ ^ $q$ = $q$ ^ $p$ 结合律：$p$ ^ ($q$ ^ $r$) = ($q$ ^ $p$) ^ $r$ 恒等律：$p$ ^ $0$ = $0$ 归零律：$p$ ^ $p$ = $0$ 自反性：$p$ ^ $q$ ^ $q$ = $q$ 很奇怪，小学很容易的接受了加法运算可以任意交换位置的事实，今天遇到的这个异或运算却非要想清楚到底为什么。然后我就想啊想啊，总结了一个规律，看对应位上 1 的个数是奇数还是偶数就好了。(在计算机中，^ 就是按位异或) p $1$ $0$ $0$ $1$ $1$ $0$ $1$ $1$ q $0$ $0$ $1$ $0$ $1$ $1$ $1$ $0$ r $1$ $1$ $1$ $0$ $0$ $1$ $1$ $0$ n = ‘1 的个数’ 2 1 2 1 2 2 3 1 n 的奇偶性 偶 奇 偶 奇 偶 偶 奇 奇 p ^ q 0 1 0 1 0 0 1 1 因为 “同为 0 ，异为 1 ” ，相异的也就只有 0 - 1 这种情况啦，而两个 1 异或得到 0 ，所以只要看 1 的个数就好了。而看个数的话，顺序当然就无所谓了。当然，这里仅仅是对于数字之间异或的小规律，严谨一点的话，还是需要从表达式来推导。 应用 首先就是之前提到的，当数组中其他的数都出现 2 次时，能够得到唯一只出现过一次的数。对这个问题进行推广，也就是，当数组中其他数都出现偶数次时，可以得到唯一出现奇数次的数。 交换两个数：python 可以直接写作 a, b = b, a，但可能有一些其他的语言不能这样写，所以使用异或还是比较有用滴，可以不需要借助中间变量。 12345a = 3b = 5a = a^b # a = 3 ^ 5b = a^b # b = 3 ^ 5 ^ 5 = 3a = a^b # a = 3 ^ 5 ^ 3 = 5 简单的数据加密，设置一个二进制串为密钥，与明文异或得到密文，与密文再次异或即得到明文。 数字校准，快速比较两数是否相同，利用了 $a$ ^ $a$ = $0$，异或为 0 时，两数相等。据说这个效率比用减法更高。 (以上～先到这，其他的，遇上再说吧……)]]></content>
      <categories>
        <category>大杂烩</category>
      </categories>
      <tags>
        <tag>异或运算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL入门学习笔记]]></title>
    <url>%2F2019%2F05%2F16%2FSQL%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"></content>
      <categories>
        <category>SQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[爬虫初体验-网站文章]]></title>
    <url>%2F2019%2F05%2F09%2F%E7%88%AC%E8%99%AB%E5%88%9D%E4%BD%93%E9%AA%8C-%E7%BD%91%E7%AB%99%E6%96%87%E7%AB%A0%2F</url>
    <content type="text"><![CDATA[日期：2019-05-09天气：没出门我也不知道……啊啊啊啊啊(请自行带入Do、Re、Mi)～今天是有点艹dan又有点满足的一天。艹dan是因为，就因为同学让我帮忙就捣鼓了一天 python 爬虫，茶不思饭不想；满足就是一天过去了还是有很多收获，也掌握了一些 python 爬虫的基本技能。 目标任务需求是爬取康安途网站医药新闻板块的所有文章，希望得到的包括每一篇文章的标题及其内容(纯文本)。大概浏览了一下页面的内容，新闻版块一共有 2317 页，每一页包含 20 篇文章，也就是实说最后会有 40k+ 条信息，我的小本本还是有点承受不住的……本来我都准被写到一个个文件里了，后来嘞，需求改了，变成把 [标题，日期，链接] 存到一个 excel 表格里，大概就酱紫。最后我还是手欠把内容给加上了。 工具安装anaconda 安装12$ conda install requests$ conda install beautifulsoup4 pip 安装12$ pip install requests$ pip install beautifulsoup4 基本思路 使用 requests + BeautifulSoup 获取网页的源代码并对其解析 使用 BeautifulSoup 中的 find 函数找到 文章名称、链接网址、发表时间 所在的 tag 使用 BeautifulSoup 中的 string/属性 从我们找到的 tag 提取需要的信息 通过之前提取出的网址，抓取该页面中的文章内容，大致步骤同上 使用 pandas 生成 excel 文件，我是感觉这个比较简便快捷 因为信息数量太大了，所以使用多个文件来保存 遇到的问题一开始由于完全没经验，直接就去网上开始搜 “python爬取网站所有文章”，还是找到了两片比较有用的，虽然说由于每个网站的结构相差比较多，不一样的地方还是多，不过因为这两篇文章还是知道了常用的库以及基本用法。 requests - BeautifulSoup 乱码问题! 叮！这里的代码是错误不完全示范！使用 requests - BeautifulSoup 解析网页源代码，最开始两篇文章中是这样的：1234url = "https://www.kangantu.com/news"req = requests.get(url)bs = BeautifulSoup(req.text)bf = bs.find_all("a", class_="tiltle") 然后就发现什么也找不到，相当于一部分内容直接丢失了……然后看了看 BeautifulSoup 的文档，借鉴了一下别人的代码。如下修改：1234567url = "https://www.kangantu.com/news"req = requests.get(url)#新增：req.encoding = 'utf-8'bs = BeautifulSoup(req.text, "html.parser")bf = bs.find_all("a", class_="tiltle") 倒腾来倒腾去，还是不对，虽然没有内容丢失，但中文部分还是会乱码。大半天被这问题困扰，终于我灵机一动，搜索 “BeautifulSoup 乱码” ，成功找到这篇文章，解决了问题，不过没有完全按照这个修改哈。总结以下嘞，就是在使用 requests + BeautifulSoup 获取源代码的时候，最好能够声明网页的编码格式，编码格式源代码里可以看到，最终如下： 1234567url = "https://www.kangantu.com/news"req = requests.get(url)#修改：req.encoding = 'gb2312'bs = BeautifulSoup(req.text, "html.parser")bf = bs.find_all("a", class_="tiltle") 如何获得需要的部分一开始因为信息丢失和乱码问题，止步不前，一发同学用 python-2.7 + urllib2 + re 解决了文章标题和链接地址的问题，但是我真的非常不喜欢用 re ，看到就晕那种，磨啊磨一直到解决了乱码问题，后面的也就迎刃而解了，如下：12345678htmlr = requests.get(url)htmlr.encoding = 'gb2312'bs = BeautifulSoup(htmlr.text, "html.parser")title_link = bs.find("div", class_="listbox").find_all("a", class_="title")date = bs.find_all("span", class_="article-date")title_list = list(item.string for item in title_link) #得到包含标题的列表link_list = list(item['href'] for item in title_link) #得到包含网址的列表date_list = list(item.string for item in date) #得到包含日期的列表 窝嘞代码最后，贴一下所有的代码～123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120# -*- coding: UTF-8 -*-"""date: 2019-5-9fucntion: get all the article of a network stationauthor: Li yinianversion: v-1.1python-3.7新增：对不完整网址的补全新增：抓取文章的内容"""#引入模块import reimport requestsfrom bs4 import BeautifulSoupimport pandas as pd#生成包含所有网址的列表def generate_url_list(page_list): """ 输入：页码范围，要求格式为 list 输出：包含要抓取信息的所页面网址的列表 """ basic_url = "https://www.kangantu.com/news/list_" url_list = [] for i in page_list: url = basic_url+str(i)+".html" url_list.append(url) return url_list#新增函数，用于转换不完整网址def url_trans(url): if url[:5] != 'https': link = "https://www.kangantu.com" + url else: link = url return link#获取页面的文章题目，时间，以及文章链接def get_title_link_date(url): """ 输入：网址链接(str) 输出：DataFrame, columns=["title", "data", "link"] """ htmlr = requests.get(url) htmlr.encoding = 'gb2312' bs = BeautifulSoup(htmlr.text, "html.parser") title_link = bs.find("div", class_="listbox").find_all("a", class_="title") date = bs.find_all("span", class_="article-date") title_list = list(item.string for item in title_link) #得到包含标题的列表 #修改：url_trans()函数补全网址 #link_list = list(item['href'] if for item in title_link) link_list_0 = list(item['href'] for item in title_link) link_list = list(map(url_trans, link_list_0)) #得到包含网址的列表 date_list = list(item.string for item in date) #得到包含日期的列表 title_date_link_df = pd.DataFrame(&#123;'title': title_list, 'date': date_list, 'link': link_list&#125;) # return title_date_link_df #新增：通过已获得的文章地址抓取文章的内容，得到包含日期的列表def get_content(link_list): """ 输入：列表-内容为已获取的文章网址 输出：对应文章内容的列表 """ content_list = [] for url in link_list: htmlr = requests.get(url) htmlr.encoding = 'gb2312' bs = BeautifulSoup(htmlr.text, "html.parser") content = bs.find("div", class_="content") content_list.append(content.get_text()) return content_list#生成excel文件def to_excel_file(file_df, file_path): """ 输入：DataFrame格式的文件，文件路径-包括名称，sheet编号 """ #writer = pd.ExcelWriter(file_path) file_df.to_excel(file_path, index=False, encoding='utf-8', sheet_name=str(file_index))def last_func(page_list, file_index): urls = generate_url_list(page_list) df = pd.DataFrame() for url in urls: title_date_link_df = get_title_link_date(url) #新增：加入文章内容 link_list = title_date_link_df['link'] title_date_link_df['content'] = get_content(link_list) df = pd.concat([df, title_date_link_df], axis=0, ignore_index=True) #修改：修改文件夹名称 to_excel_file(df, "/home/may/daydayup/kangantu-more/kangantu-news-"+str(file_index)+".xls")#正式的代码部分，网站需要爬取的共 2317 页-rang(1,2318)list_n = list(range(1, 2318, 50))list_n.append(2318)page_hole_list = [list(range(list_n[i-1], list_n[i])) for i in range(1, len(list_n))]for i in range(len(page_hole_list)): file_index = i page_list = page_hole_list[i] last_func(page_list, file_index+1)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.model_selection]]></title>
    <url>%2F2019%2F05%2F08%2Fsklearn-model-selection%2F</url>
    <content type="text"><![CDATA[整理sklearn.model_selection中一些常用的类及其基本用法。 拆分数据(sklearn.model_selection.train_test_split)用于将数据集拆分为两部分，一部分用于模型训练，一部分用于模型评估。 train_test_split(*arrays, test_size=0.25, train_size=None, random_state=None, shuffle=True, stratify=None)*arrays 相同长度的一系列n个数据集，接受格式有[lists, numpy arrays, scipy-sparse matrices or pandas dataframes]。返回2n个数据集，一个输入对应两个输出。test_size=0.25 整数或小数。代表个数或比例train_size=None 同上，两个_size指定一个就好了random_state=None 随机数种子shuffle=True 拆分前是否打乱数据。If shuffle=False then stratify must be None.stratify=None 12from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) 网格搜索(sklearn.model_selection.GridSearchCV)这个呢，hei好用，可以便捷的帮我们确定最优参数，并且设置好需要训练的模型还有对应的参数，会在每一组参数进行训练，然后使用最优的一组参数来训练最终的模型，设置起来也不复杂。 GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=None, iid=’warn’, refit=True, cv=’warn’, verbose=0, pre_dispatch=’2*n_jobs’, error_score=’raise-deprecating’, return_train_score=’warn’)estimator 学习器接口，设定用于训练模型的算法，比如 sklearn.svm.SVR()。需要有 scoring 参数，否则不对模型进行评估param_grid 参数网格，传入字典或 value为列表的字典。e.g. {‘C’: [1, 10, 100]}scoring=None 模型的评分方式，如果为 None，则使用模型默认的度量。可以是字符串、可调用对象、列表 / 元组、字典。fit_params=None 传给fit方法的参数n_jobs=None 并行作业数iid=&#39;warn&#39; (Changed in version 0.20: Parameter iid will change from True to False by default in version 0.22, and will be removed in 0.24)refit=True 是否在整个数据集上使用得到的最佳参数重新训练。cv=&#39;warn&#39; 交叉验证策略，默认为 3折交叉验证。verbose=0 控制冗余，越高，信息越多。pre_dispatch=&#39;2*n_jobs&#39; 控制并行作业期间分配的作业数量，可以是 int、str(关于 n_jobs的函数表达式)。error_score=&#39;raise-deprecating&#39; 如果训练出错返回的分数。return_train_score=&#39;warn&#39; 如果 False，cv_results_属性不包含分数。 123456789101112131415from sklearn.model_selection.GridSearchCVfrom sklearn.svm import SVCmodel = SVC()param_range = &#123;'C': [1, 10, 100]&#125;clf = GridSearchCV(model, param_range, scoring='roc_auc', cv=10, verbose=1, n_jobs=4)clf.fit(X_train, y_train)y_pre = clf.predict(X_test)clf.score(X_test, y_test)clf.cv_results_ #可以转化为DataFrame格式的字典，keys是列名称，values是对应的列clf.best_score_ #最高得分clf.best_params_ #最佳的一组参数clf.refit_time_ #在整个数据集上使用最优参数训练模型所花的时间 交叉验证(sklearn.model_selection.cross_val_score)可以返回值为每一次交叉验证后得分的数组，每一次只能针对一组参数值进行训练，所以如果要确定最优的参数，得自己写一个循环，比较麻烦，得到最优参数之后还要重新在所有数据上自己训练一遍…… cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=’warn’, n_jobs=None, verbose=0, fit_params=None, pre_dispatch=’2*n_jobs’, error_score=’raise-deprecating’)X 需要训练的数据y=None 数据的标签值groups=None 划分数据为 train/test set 时使用的分类标签(其余同上) 123456789101112131415from sklearn.model_selection import cross_val_scorefrom sklearn.svm import SVCimport numpy as npC_list = [1, 10, 100]cv_scores = []for c_value in C_list: model = SVC(C=c_value) score = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=10, verbose=1, n_jobs=4) #score是一个数组哦哦哦 cv_score = np.means(score) cv_scores.append(cv_score) best_c = C_list[cv_scores.index(max(cv_scores))]...... 验证曲线(sklearn.model_selection.validation_curve)这个函数嘞，会返回两个数组：train_scores、test_scores ，比较方便用来画针对某一个参数的学习曲线。两个分别是在进行交叉验证时在训练集s以及验证集上的得分，大小为 (传入的参数数量，交叉验证折数) validation_curve(estimator, X, y, param_name, param_range, groups=None, cv=’warn’, scoring=None, n_jobs=None, pre_dispatch=’all’, verbose=0, error_score=’raise-deprecating’)param_name 变化的参数的名称param_range array-like，用于训练的参数值，对应参数名称 123456789101112131415from sklearn.model_selection import validation_curvefrom sklearn.tree import DecisionTreeClassifierimport matplotlib.pyplot as pltparam_range = list(range(1:9))train_score, test_score = validation_curve(DecisionTreeClassifier(), X_train, y_train, param_name='max_depth', param_range=param_range, cv=10, scoring='roc_auc')train_score = np.mean(train_score, axis=1)test_score = np.mean(test_score, axis=1)plt.plot(param_range, train_score, 'o-',color='r', label='training')plt.plot(param_range, test_score, 'o-',color='g', label='testing')plt.legend(loc='best')plt.xlabel('depth')plt.ylabel('AUC measurement')plt.show()]]></content>
      <categories>
        <category>sklearn</category>
      </categories>
      <tags>
        <tag>数据拆分</tag>
        <tag>交叉验证</tag>
        <tag>网格搜索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小象学院《python人工智能》课程总结]]></title>
    <url>%2F2019%2F04%2F28%2F%E5%B0%8F%E8%B1%A1%E5%AD%A6%E9%99%A2%E3%80%8Apython%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%8B%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[最近在小象学院买了个课《Python人工智能》，课程涉及了一些基本的机器学习算法，也包含了几个机器学习的实战项目，虽然都比较基础，但是从中还是可以学到很多东西，今天天阴，正好就一起好好整理一下吧。嗯……先整体归纳一下，在这门课程当中学到的东西，以及自己的一些感悟，再就每个不同的项目进行简单的分析。 先瞎叨叨几句开始的开始，我们都是孩子……噗噗噗，stop，说正事！1.机器学习的整个过程大致可以分为：数据收集、数据处理、构建模型、训练模型、预测结果。共五个部分。在这次课程的大作业中每一个项目都包含了除数据收集的其余四个过程。在真正完成一个项目时，由于对预测结果的追求，肯定不是说五个过程顺序走一趟就可以了，而总是一个循环往复的过程，不断尝试、不断改进，直到最终模型的预测结果达到让人满意的程度。2.数据处理需要把收集到的人能读懂的数据，通过一定的方法和规则，转换成能够运算的数字信息。一般来说都是对其进行分类编码。在这一部分还可能要做的就是特征的取舍，因为并不是所有的特征都与结果相关。此外，收集到的数据很有可能包含缺失值，对于缺失值，可以选择放弃或是填充，在包含缺失值样本极少时可以放弃不完整样本；又或者，有某一个特征包含大量的缺失值，也可以抛弃这个特征；而填充策略则有：向上填充、向下填充、均值填充、中位数填充等。在这一部分啊啊啊，还可以通过图像来快速判断特征之间，特征与标签之间的关系，为之后的模型建立作准备。总之呢，就是把手头的数据处理的能够直接上算法使用，而且还要对特征与预测结果的关系心中有数(大概吧)。3.构建模型不是随便拿一个算法模型套上去就好了，而是要根据对数据的把握和对不同模型的理解，尽可能的选择适合的一个或者几个模型。在实际情况中，构建模型和训练模型往往是分不开的，因为大部分模型的都包含有超参数，为了选择最好的超参数，和最好的模型，当然就需要在数据集上进行多次训练最后取其中最好的结果啦。4.如果已经得到了满意的模型，那么只要将数据处理的过程和训练好的模型保存起来就可以对新的数据进行预测。 就每个作业仔细叨叨一共五个大作业，分别为：IBM员工流失预测、贷款审批结果预测、员工流失预测进阶、蘑菇聚类分析、泰坦尼克幸存者预测。 IBM员工流失预测该项目属于监督学习中的分类问题，主要目的是预测在未来一段时间公司的员工是否可能会离职。收集到的数据包括每一名员工的年龄、出差频率、职位等等信息，还包括作为标签列的是否离职信息。样本特征中既有数值型特征，也有字符型的标签特征。原作业点这里因为这是第一个大作业，咱就梳理的仔细些，先来看看整个过程中都做了些什么吧： 引入可能用到的库、类或者函数，然后导入数据。 人工观察数据(这时候就需要一双慧眼和经验啦)，搞明白每一列数据都是干嘛的，数据跟标签大概是个什么关系(一般领域靠常识还是能基本get到的)。顺便一行代码看看有没有空值，如果有就需要采取行动了，这个项目没有空值直接跳过。 这里使用了seaborn.pairplot()对可能与标签值相关的一些特征(n个)与特征之间的关系进行绘图(n×n)，方便快速查看特征对标签的影响。 单独用一个变量y来存放标签值；把数据集中的数值列和非数值列区分开来，将两种不同数据类型的特征名称存储在两个列表中(方便直接提取数据啊啦啦)，便于之后的数据处理。 特征处理，用pandas.get_dummiees()对非数值特征进行了独热编码(这个函数是真的好用)。 用sklearn.model_selection.train_test_split()拆分数据集，1/5用作测试，然后应用scikit-learn中的模型对数据进行训练，因为是二分类问题，可以选择的模型比较多，这里选了五种：高斯朴素贝叶斯、决策树、KNN、支持向量机、逻辑回归。 在训练过程中，用for循环对每个模型的超参数都尝试了几个不同的值，并分别用列表记录了对应的准确率。 通过条形图直观比较不同模型的最优准确率。最后还用F1值(这个会更准确)对几个模型进行了评估。 (大概过程都差不多了，之后就主要针对数据处理过程和模型的选择进行阐述啦) 贷款审批结果预测依然是一个二分类问题，需要模型的输出为是否同意申请人的贷款。原作业点这里数据处理： 用pandas.DataFrame.describe()简单统计了特征信息(这里得到的只有数值列信息)，发现有缺失值。 重复值处理，根据ID判断，搜集到的数据中是否有重复样本。 缺失值处理，统计每一个特征分别有多少个缺失值，然后直接把有空值的样本删除了(好任性啊……)。 特殊值处理，把Dependents列的3+全部转换为3 。 根据特征数据的类型将所有特征分为了3类：数值型，有序型(比如受教育程度)，类别型。将标签数据转换为数值型。 特征处理，对有序型特征进行标签编码，类别型特征进行独热编码(作业当中是先标签编码再独热编码，但是没必要这样啦)，数值型数据进行了归一化处理。 把所有处理好的特征再合并起来。 模型选择： 使用网格搜索（GridSearchCV）来调整模型的重要参数，这里写了一个函数：输入训练数据、测试数据、模型、参数，输出训练好的最优模型、最优模型在测试集上的得分、训练时间。之后直接调用就好啦。 定义模型参数字典，关键字为模型名称，值为元组：(模型，参数字典)。通过for循环调用写好的函数对四种模型(KNN、决策树、逻辑回归、SVM)进行训练，返回值存储为DataFrame格式(方便之后作图比较)。 比较了不同模型的准确率和训练耗费的时间。 (这里的过程也是非常详细啦，所以再之后我们就只罗列一些值得注意学习的点了) 员工流失预测进阶(数据和项目简介之前已经有了，不再赘述)原作业点这里这里分别对数值型数据进行了归一化/标准化，比较了两种不同操作的训练结果；利用sklearn.model_selection.validation_curve绘制了学习曲线。叮！当样本分布不咋均衡的时候，可以对样本进行重采样(pandas.DataFrame.sample()) 。重采样有两种策略，一种是向下采样(在我们的数据中，分类0的数据比分类1多，向下采样即将分类0的数据量缩减到跟分类1相等attrition_class_0.sample(count_class_1))；另一种则是向上采样(向上采样即将分类1的数据量随机增加到跟分类0相等attrition_class_1.sample(count_class_0, replace=True))。【选修】Python 的 imbalanced-learn 模块提供了更为丰富和科学的重采样方法。尝试使用imblearn.combine.SMOTETomek 来做上下采样相结合的数据处理。 蘑菇聚类分析数据集包含了蘑菇的各种特征共22个，比如帽形、帽面、气味等等，所有的特征数据都是字符型的。原作业点这里通过条形统计图直观展示了每个特征都有几类，以及每一类的数量，可以筛掉那些所有样本都相同的特征。ps.因为这种没有特色的，用了也是白搭啊哈哈。虽然都是类别型数据，但也采取了区别对待。对仅包含2个可能值的变量使用一个简单的标签编码，对包含3个或多个可能值的变量进行热编码。叮！使用sklearn.decomposition.PCA对特征进行了降维处理，在尽可能保留数据信息的同时减少特征的数量。 泰坦尼克幸存者预测终于终于到这个非*儿有名的沉船问题了，然后就一起来看看什么样的人群在这次沉船事件中更容易活下来吧。原作业点这里画图分析了每一个特征内的存活率(比如性别这一特征中，男性的存活率)。然后嘞，从这个图也能反映一些问题，比如说：不是每个人的仓位号都是独一无二的，说明这个特征竟然还有跟最后的结果有点关系的。观察同一个仓位(比如E44)中的乘客信息，看名字发现他们还是有点关系的，然后把每一个人名字的Title提取了出来(比如Mr、Miss)，有些title出现的次数很少，就把他们统一归位了一类-Misc 。 终极大盘点后来的后来，渴望变成天使…… 基本流程这里不想瞎叨叨，先用一个流程图来理一理吧！(都能画流程图了，感觉自己很nb的样子吼^*-*^)吭哧吭哧写好了代码，结果发现太长了，嘤嘤嘤，还是换图片吧。ps. 这里提到的处理方法都很基础，不过嘞，这本身也只是对大作业的一个总结，理一理大概的过程，机器学习那么多方法，全部罗列也写不完啊…… 方法盘点1234import pandas as pdimport numpy as npimport seaborn as sns import matplotlib.pyplot as plt 导入数据：12file_path = '(这里面是文件的路径以及名字。csv)'data = pd.read_csv(file_path) 作图分析：1234567891011plt.figure()#可视化属性之间关系sns.pairplot(data[numerical], hue='Attrition_numerical', palette='seismic', diag_kind = 'kde', diag_kws=dict(shade=True))#柱形图data.plot.bar()data.plot(x=column_name, y=['Survival Rate'], kind='bar', title=column_name, legend=False)#eheheha，其他的就不放在着了吧啦啦啦...... 数据处理：12345678910111213141516171819202122232425262728293031323334353637383940414243#是否存在重复样本？(这里是根据ID判断)data[data['ID'].duplicated()].shape[0] == 0#是否有空值？data.isnull().any() #会返回每一列是否有空值，bool型#丢掉有空值的样本data = data.dropna()#空值填充#Method.1from sklearn.preprocessing import Imputerimputer = Imputer(missing_values=’NaN’, strategy=’median’, copy=True)data = imputer.fit_transform(data)#Method.2data.fillna(value=None) #直接指定填充值data.fillna(method='backfill') #指定填充方案#重采样data.sample(sample_number)#数据编码#独热编码#sklaernfrom sklearn.preprocessing import OneHotEncoderencoder = OnrHotEncoder(sparse=False)data = encoder.fit_transform(data)#pandasdata = pd.get_dummies(data) #这个函数可以保持原来的数据格式#标签编码from sklearn.preprocessing import LabelEncoderencoder = LabelEncoder(sparse=False) #每次只能对单一特征编码data = encoder.fit_transform(data)#数据降维from sklearn.decomposition import PCApca = PCA(n_components=None) #指定一个数(整数或小数)，可能小数会比较方便data = pca.fit_transform(data)n_components = pca.n_components_ #返回主成分数量#数据拆分from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.2, random_state=1) 模型训练：1234567891011121314151617181920212223242526272829303132333435#网格搜索from sklearn.model_selection import GridSearchCVclf = GridSearchCV(estimator = model,param_grid = param_range, cv = 6, scoring = 'roc_auc', refit = True, verbose = 1, n_jobs = 4)clf.fit(X_train, y_train) #训练后可以得到最优的模型best_param = clf.best_param_ #返回训练得到的最优参数best_estimator = clf.best_estimator_train_score = clf.score(X_train, y_train)test_score = clf.score(X_test, y_test)#监督学习from sklearn.neighbors import KNeighborsClassifier #KNNfrom sklearn.tree import DescisionTreeClassifier #决策树-分类from sklearn.tree import DescisionTreeRegress #决策树-回归from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB #朴素贝叶斯from sklearn.linear_model import LinearRegression #线性回归from sklearn.linear_model import LogisticRegression #逻辑回归from sklearn.svm import SVC #SVM-分类from sklearn.svm import SVR #SVM-回归#非监督学习from sklearn.cluster import KMeans #K均值算法#绘制学习曲线from sklearn.model_selection import validation_curvetrain_score, test_score = validation_curve(DecisionTreeClassifier(), X_train, y_train, param_name='max_depth', param_range=param_range, cv=10, scoring='roc_auc')train_score = np.mean(train_score,axis=1)test_score = np.mean(test_score,axis=1)plt.plot(param_range,train_score,'o-',color = 'r',label = 'training')plt.plot(param_range,test_score,'o-',color = 'g',label = 'testing') 导出数据：123y_pre = model.predict(data_to_pre)result = pd.DataFrame(&#123;'ID': list(range(len(ypre))), 'pre_value': y_pre&#125;)result.to_csv('文件路径及名字')]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sklearn入门-非监督学习]]></title>
    <url>%2F2019%2F04%2F22%2Fsklearn%E5%85%A5%E9%97%A8-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[对不带标签的数据进行学习。 k均值算法(k-Means)(sklearn.cluster.KMeans)最基本的聚类算法。 KMeans(n_clusters=8, init=’k-means++’, n_init=10, max_iter=300, tol=0.0001, precompute_distances=’auto’, verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm=’auto’)n_clusters=8 聚类数。init=&#39;k-means++&#39; 初始化聚类中心方法。/ ‘auto’ / 数组shape (n_clusters, n_features)n_init=1 算法选择不同聚类中心运行的次数，保留最好的结果。max_iter=300 单次运行的最大迭代数目。tol=0.0001 停止迭代的相对误差界限。precompute_distances=&#39;auto&#39; {‘auto’, True, False}是否预先计算距离，选择’auto’时，当n_samples * n_clusters&gt;1,000,000 则不预先计算。verbose=0 intrandom_state=None 如果是int，则random_state是随机数生成器使用的种子; 如果是RandomState实例，则random_state是随机数生成器; 如果为None，则随机数生成器是np.random。copy_x=True 是否拷贝训练数据。n_jobs=None 并行作业数。algorithm=&#39;auto&#39; “auto”, “full” or “elkan”. 指定要使用的算法。 12345from sklearn.cluster import KMeanskm_model = KMeans(n_clusters=8, random_state=None)km_model.fit_transform()centers = km_model.cluster_centers_ #聚类中心y_pre = km_model.labels_ #每个样本点的标签 自底向上层次聚类(sklearn.cluster.AgglomerativeClustering)层次聚类可分为两类：自顶向下, diverse；自底向上, agglomerative。 AgglomerativeClustering(n_clusters=2, affinity=’euclidean’, memory=None, connectivity=None, compute_full_tree=’auto’, linkage=’ward’, pooling_func=’deprecated’)n_clusters=2 最终的聚类数。affinity=&#39;euclidean&#39; {‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’, ‘cosine’, ‘precomputed’}. 用于计算距离，当linkage=’ward’，只支持’euclidean’。memory=None 默认不缓存计算树的输出，以字符指定缓存路径。connectivity=None 一个数组或者可调用对象或者None，用于指定连接矩阵compute_full_tree=&#39;auto&#39; 通常当训练了n_clusters后，训练过程就会停止，但是如果compute_full_tree=True，则会继续训练从而生成一颗完整的树。linkage=&#39;ward&#39; 一个字符串，用于指定链接算法。’ward’：单链接； ‘complete’：全链接； ‘average’：均连接。pooling_func=&#39;deprecated&#39; 一个可调用对象，它的输入是一组特征的值，输出是一个数 12from sklearn.cluster import AgglomerativeClusteringagg_cluster = AAgglomerativeClustering(n_clusters=2, linkage='ward')]]></content>
      <categories>
        <category>sklearn</category>
      </categories>
      <tags>
        <tag>非监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn入门-监督学习]]></title>
    <url>%2F2019%2F04%2F22%2Fsklearn%E5%85%A5%E9%97%A8-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[这篇文档只对sklearn关于几种基本的监督学习算法进行简单的阐述汇总，不涉及参数详细含义。在本文代码块中，定义模型的括号内为一般需要调整的参数(给出的为原始值)。 k近邻算法(kNN)(sklearn.neighbors.KNeighborsClassifier) KNeighborsClassifier(n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, metric_params=None, n_jobs=None, **kwargs)n_neighbors=5 分类数目，这个参数一般都要自己设置啊啊啊。weights=&#39;uniform&#39; 权重，默认为’uniform’类别中的所有点权重相同。支持’distance’，此时权重时距离的倒数，离分类点越近，权重越大；也可传入一个自己定义的函数名，要求接受一个距离数组，并返回一个包含权重的相同形状的数组。algorithm=&#39;auto&#39; 用于计算最近分类点的算法，默认’auto’尝试根据传递给fit方法的值来确定最合适的算法。其他：’ball_tree’将使用BallTree，’kd_tree’将使用KDTree，’brute’将使用暴力搜索。leaf_size=30 传递给BallTree或KDTree的叶子大小。 这可能会影响构造和查询的速度，以及存储树所需的内存。 最佳值取决于问题的性质。p=2 整数。Minkowski距离度量的参数。 当p = 1：曼哈顿距离(L1)；p = 2：欧几里得距离(L2)。对于任意p，使用minkowski_distance(Lp)。metric=&#39;minkowski 距离度量函数。metric_params=None 度量函数的其他关键字参数n_job=None 并行作业数 12345678910111213141516#导入kNNfrom sklearn.neighbors import KNeighborsClassifier"""以下四步即为sklearn用于模型训练，数据预测，模型评估的一般基本使用方法"""#定义模型kNN_model = DescisionTreeClassifier(n_neighbors=5) #训练模型kNN_model.fit(X_train, y_train)#使用训练好的模型进行预测y_predict = kNN_model.predic(X_test)#使用测试集对模型进行评分kNN_score = kNN_model.score(y_test, y_predict) 决策树(sklearn.tree) 分类树(sklearn.tree.DecisionTreeClassifier) DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)criterion=&#39;gini&#39; 特征选择标准。’gini’: 基尼指数，’entropy’: 信息增益splitter=&#39;best&#39; 每个节点选择如何分类的策略。’best’/‘random’max_depth=None 最大深度（深度小可提高泛化能力,避免过拟合）min_samples_split=2 拆分内部节点所需的最小样本数。int(个数)/float(比例)min_samples_leaf=1 每一个叶子节点的最小样本数。min_wight_fraction_leaf=0.0 每一个叶子节点的最小样本权重和，小于该值会被剪枝。max_faetures=None 寻找最佳分类特征时考虑的分类数量。’auto’/‘sqrt’: sqrt(n_features); ‘log2’:log2(n_features); None:n_featuresrandom_state=None 如果是int，则random_state是随机数生成器使用的种子; 如果是RandomState实例，则random_state是随机数生成器; 如果为None，则随机数生成器是np.random。max_leaf_nodes=None 最大叶子节点数，防止过拟合min_impurity_decrease=0.0 如果该分裂导致不纯度的减少大于或等于该值，则将分裂节点。min_impurity_split=None 树提前停止生成的阈值，若某节点的不纯度小于/等于该值，停止分裂class_weight=None 指定样本各类别的的权重，防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。可以自己指定各个样本的权重，或者用“balanced”，如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。如果样本类别分布没有明显的偏倚，可以不管这个参数，选择默认的”None”。{class_label: weight}presort=False 是否提前排序。 回归树(sklearn.tree.DecisionTreeRegressor) DecisionTreeRegressor(criterion=’mse’,splitter=’best’,max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None, presort=False)criterion=&#39;mse&#39; 误差计算。 ‘mse’: 均方误差，’friedman_mse’: L2，’mae’: L1(其余同上) 1234567#分别导入分类树和回归树from sklearn.tree import DecisionTreeClassifierfrom sklearn.tree import DecisionTreeRegressor#定义模型DTC_model = DescisionTreeClassifier(max_depth=None, random_state=None)DTR_model = DescisionTreeRegressor(max_depth=None, random_state=None)...... 朴素贝叶斯(sklearn.naive_bayes)高斯模型：GaussianNB(priors=None, var_smoothing=1e-09)，适用于连续值。伯努利模型：BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)，适用于离散值。多项式模型：MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)，适用于离散值。 priors=None 类的先验概率var_smoothing=1e-09 为使计算稳定添加的特征最大方差的部分(？)alpha=1.0 平滑参数binarize=0.0 样本特征的二值化（映射到布尔值）的阈值。 如果为None，则假定输入已包含二进制向量。fit_prior=True 是否学习先验概率class_prior=None 类的先验概率(目前这几种模型，不调参，对这些参数的理解还不够清晰透彻) 线性回归(sklearn.linear_model.LinearRegression)线性回归主要用于对连续值的预测。该模块在未对样本进行特别处理时，只能对数据进行简单的线性拟合，但配合其他一些数据处理模块却能够进行更复杂的曲线拟合等，此处暂不过多介绍。 LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)fit_intercept=True 训练时是否考虑截距。 如果设置为False，则不会在计算中使用截距(e.g. 预计数据已经居中)。normalize=False 当fit_intercept设置为False时，将忽略此参数。 如果为True，则回归量X将在回归之前通过减去平均值并除以L2范数来归一化。copy_X=True 如果为True，则将复制X; 否则，它可能会被覆盖。n_jobs=None 并行作业数。 逻辑回归(sklearn.linear_model.LogisticRegression)逻辑回归用于处理二分类问题，多个分类器组合也可用于多分类问题。在应用于多分类问题时，需要softmax(其实我觉着也可以不需要，直接选值最高的就行，不过处理之后可以让不同类的预测值为1，近似于概率)。 LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)penaity=&#39;l2&#39; 用于指定惩罚项(正则项)。’l2’(默认)或者’l1’，L2很好用，一般别动它dual=False 对偶或者原始方法。Dual只适用于正则化相为l2 liblinear的情况，通常样本数大于特征数的情况下，默认为False。tol=0.0001 停止训练的误差值大小。C=1 C为正则化系数λ的倒数，通常默认为1。C越大，正则化系数越小，一般设置它小于1fit_intercept=True 是否存在截距，默认存在intercept_scaling=1 仅在正则化项为”liblinear”，且fit_intercept设置为True时有用。class_weight=None 类的权重。dict / ‘balanced’random_state=None 如果是int，则random_state是随机数生成器使用的种子; 如果是RandomState实例，则random_state是随机数生成器; 如果为None，则随机数生成器是np.random。solver=&#39;warn&#39; str, {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}.默认：’liblinear’其中，’liblinear’: 适用于小数据集，仅可用于二元分类。&lt;’newton-cg’: ‘lbfgs’: ‘liblinear’: ‘sag’: ‘saga’:&gt;max_iter=&#39;100&#39; 求解器收敛的最大迭代次数。仅适用于newton-cg，sag和lbfgs求解器。multi_class=&#39;warn&#39; str，{‘ovr’，’multinomial’，’auto’}，默认值：’ovr’。verbose=0 对于liblinear和lbfgs求解器，将详细设置为任何正数以表示详细程度。warm_start=False 设置为True时，重用上一次调用的解决方案以适合初始化，否则，只需擦除以前的解决方案。对于liblinear解算器无效。n_jobs=None 并行作业数。 1234from sklearn.linear_model import LogisticRegression#定义模型及常用参数RL_model = LogisticRegression(C=1, randomstate=None)...... 支持向量机(SVM)(sklearn.svm)sklearn中的SVM模块既可用于分类问题(SVC)，也可以用于回归问题(SVR)。 用于分类问题(sklearn.svm.SVC) SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto_deprecated’, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)C=1.0 正则项系数的倒数kernel=&#39;rbf&#39; 算法使用的核函数。必须是’linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’之一或者可传入对象degree=3 多项式核函数的次数(‘poly’)。对其余核函数忽略gamma=’auto_deprecated’ ‘rbf’,’poly’ 和’sigmoid’的核函数参数。默认是’auto’，会选择1/n_features。coef0=0.0 核函数的常数项。对于’poly’和’sigmoid’有校。shrinking=True 是否采用shrinking heuristic方法，默认为trueprobability=Falsetol=0.001 停止训练的误差值大小，默认为1e-3cache_size=200 核函数cache缓存大小，默认为200class_weight=None 类别的权重，字典形式传递。verbose=False 是否允许冗余输出。max_iter=-1 最大迭代次数。-1为无限制。decision_function_shape=’ovr’ ‘ovo’ / ‘ovr’random_state=None 如果是int，则random_state是随机数生成器使用的种子; 如果是RandomState实例，则random_state是随机数生成器; 如果为None，则随机数生成器是np.random。 用于回归问题(sklearn.svm.SVR) SVR(kernel=’rbf’, degree=3, gamma=’auto_deprecated’, coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)epsilon=0.1 Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.(其余同上) 12345from sklearn.svm import SVC #分类from sklearn.svm import SVR #回归svc_model = SVC(C=1.0, kernel='rbf', degree=3, random_state=None)svr_model = SVR(kernal='rbf', C=1) (完)]]></content>
      <categories>
        <category>sklearn</category>
      </categories>
      <tags>
        <tag>监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn入门-数据预处理]]></title>
    <url>%2F2019%2F04%2F20%2Fsklearn%E5%85%A5%E9%97%A8-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[sklearn.preprocessing 数据填充(sklearn.preprocessing.Imputer)收集到的数据总会遇到一些残缺值，如果不想丢弃这个样本，就只能想办法最数据进行填充了，一般的填充方法有：向上填充、向下填充、均值填充、中位数填充等等。填充数据可以使用sklearn中的类，也可以使用pandas里面的方法～ Imputer(missing_values=’NaN’, strategy=’mean’, axis=0, verbose=0, copy=True)missing_values=&#39;NaN&#39; 用于匹配缺失值strategy=&#39;mean&#39; 填充策略，[‘mean’, ‘median’, ‘most_frequent’]，均值、中位数、众数。axis=0 填充方向，默认为列verbose=0 控制imputer的详细程度copy=True 是否拷贝 数据标准化(skleaarn.preprocessing.StandardScaler)z = (x - u) / s ，转换使得数据均值为0，方差为1。 StandardScaler(copy=True, with_mean=True, with_std=True)with_mean=True 若为False，则 u=0with_std=True 若为False，则 z=1 数据归一化(sklearn.preprocessing.MinMaxScaler)缩放数据到一定范围。 MinMaxScaler(feature_range=(0, 1), copy=True) 独热编码对非数值型的数据进行编码，根据数据每一特征的值的种类，将其生成向量。1～k sklearn.preprocessing.OneHotEncoder() OneHotEncoder(n_values=’auto’, categorical_features=’all’, categories=’auto’, dtype=&lt;class‘numpy.float64’&gt;, sparse=True, handle_unknown=’error’)n_values 每个特征使用几维数据，默认由数据集自动决定categorial_features 指定对哪些特征进行编码，默认为传入的所有值，通过bool值或索引进行指定(e.g. [True, True, False] / [0, 1])categories 每个特征的类别dtype 编码数值格式sparse 默认返回稀疏矩阵，设置为False可直接返回array，否则需要.toarray()转换handle_unknown ‘error’/‘ignore’，遇到未知类别，返回错误/忽略 12345678910from sklearn.preprocessing import OneHotEncoderOneHot_enc = OneHotEncoder(sparse=False)#第一次对数据进行编码直接使用 fit_transform()data1_coded = OneHot_enc.fit_transform(data1)#对相同结构数据集再次编码，仅需 transform()data2_coded = OneHot_enc.transform(data2)array.reshape(-1, 1) #数据仅包含一个特征，转化为n×1维矩阵array.reshape(1, -1) #数据仅包含一个样本，转化为1×n维矩阵 使用OneHotEncoder编码后，返回为一个数组(np.array)，且编码后的数据会丢失原来的列名称。 pandas.get_dummies()get_dummies()对数据进行编码后，返回 pd.DataFrame ，并且可以根据特征中不同的值自动生成列名称。 get_dummies(data, prefix=None, prefix_sep=’_’, dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)prefix 列表或这字符串用于添加列名称prefix_sep 字符串/列表/字典，基于prefix，用作分隔符dummy_na 默认’False’忽略空值，设置为’True’则新增列用于指示空值columns 需要编码的列名称drop_first 是否删除特征的第一类 12import pandas as pddata_coded = pd.get_dummies(data) #不改参数就可以表现的很优秀啦 标签编码(sklearn.preprocessing.LabelEncoeder)对非数值型的数据进行编码，根据数据每一特征的值的种类，将其生成向量。1～1！每次只能对单一特征编码，所以需要的情况下，得自己写一个for循环。 特征降维-主成分分析法PCA(sklearn.decomposition.PCA)特征降维能在尽可能多的保留数据信息的情况下减少特征的数量，在数据样本不足而特征值过多的时可以是模型得到较好的解，还能提高模型泛化能力……此外，还能减少模型的训练成本，加快运算速度。 PCA(n_components=None, copy=True, whiten=False, svd_solver=’auto’, tol=0.0, iterated_power=’auto’, random_state=None)n_components=None 期望保留的主成分个数。为整数时，即为保留的个数；若为小于1的正整数，则保留使得方差百分比大于该值的最少数目的成分。还可以传入字符型参数，比如’mle’，将自动选取特征个数n，使得满足所要求的方差百分比。copy=True 是否复制原始数据。whiten=False 白化，是否要使得每个特征具有相同的方差。svd_solver=&#39;auto&#39; string {‘auto’，’full’，’arpack’，’randomized’}.(老实说，下面这一段是谷歌翻译得来的，还没有时间细究，因为我也还不太明白)auto：解析器由基于X.shape和n_components的默认策略选择：如果输入数据大于500x500且要提取的组件数低于数据最小维数的80％，那么效率更高’随机化’方法已启用。否则，计算精确的完整SVD并随后截断。full：运行完全完整的SVD通过scipy.linalg.svd调用标准LAPACK解算器并通过后处理选择组arpack：运行SVD截断为n_components通过scipy.sparse.linalg.svds调用ARPACK解算器。它严格要求0 &lt;n_components &lt;min（X.shape）随机的：通过Halko等方法进行随机SVD。tol=0.0 由svd_solver ==’arpack’计算的奇异值的容差iterated_power=&#39;auto&#39; int&gt; = 0，或’auto’，由svd_solver ==’randomized’计算的幂方法的迭代次数。random_state=None (略) 1234567from sklearn.decomposition import PCApca = PCA(n_components=None) #这里最好要指定一个数的啊，可能小数会比较方便pca.fit_transform(data) #对数据进行训练并降维components = pca.components_ #可获得方差比由大到小排列的所有主成分var = pca.explained_variance_ #转换后个主成分的方差var_atio = pca.explained_variance_ratio_ #转换后个主成分的方差比n_components = pca.n_components_ #可返回需要的特征数 (完)]]></content>
      <categories>
        <category>sklearn</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
      </tags>
  </entry>
</search>
